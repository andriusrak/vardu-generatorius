{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf2zA28i+A/U7Yg9/msJK9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andriusrak/vardu-generatorius/blob/main/Vardugeneratorius2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l7soc7-nHkmg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "\n",
        "names_vyro = []\n",
        "names_moters = []\n",
        "for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "            'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
        "    url_vyro = f'https://vardai.vlkk.lt/sarasas/{key}/?lytis=vyro&kilme='\n",
        "    url_moters =f'https://vardai.vlkk.lt/sarasas/{key}/?lytis=moters&kilme='\n",
        "\n",
        "    #gaunam vyrisku vardu list\n",
        "    response = requests.get(url_vyro)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    vyro_links = soup.find_all('a', class_='names_list__links names_list__links--man')\n",
        "    names_vyro += [name.text for name in vyro_links]\n",
        "\n",
        "    #gaunam moterisku vardu list\n",
        "    response = requests.get(url_moters)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    moters_links = soup.find_all('a', class_='names_list__links names_list__links--woman')\n",
        "    names_moters += [name.text for name in moters_links]\n",
        "\n",
        "\n",
        "np.savetxt('vyru_vardai.txt', names_vyro, fmt='%s', header='name', comments='', newline='\\n')\n",
        "np.savetxt('moteru_vardai.txt', names_moters, fmt='%s', header='name', comments='', newline='\\n')\n",
        "\n",
        "#sujungtas failas\n",
        "all_names = names_vyro + names_moters\n",
        "np.savetxt('vardai.txt', all_names, fmt='%s', header='name', comments='', newline='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "IFUTN2MaHpWr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NameDataset(Dataset):\n",
        "    def __init__(self, male_file, female_file):\n",
        "        # Read both male and female names\n",
        "        male_names = pd.read_csv(male_file)['name'].values\n",
        "        female_names = pd.read_csv(female_file)['name'].values\n",
        "\n",
        "        # Combine names with gender labels (0 for male, 1 for female)\n",
        "        #self.names = [(name, 0) for name in male_names] + [(name, 1) for name in female_names]\n",
        "        #\n",
        "        # Lowercase raides padarom\n",
        "        self.names = [(name.lower(), 0) for name in male_names] + [(name.lower(), 1) for name in female_names]\n",
        "\n",
        "\n",
        "\n",
        "        # Create character vocabulary\n",
        "        all_names = [name for name, _ in self.names]\n",
        "        self.chars = sorted(list(set(''.join(all_names) + ' ')))\n",
        "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name, gender = self.names[idx]\n",
        "        name = name + ' '  # Adding padding character\n",
        "        encoded_name = [self.char_to_int[char] for char in name]\n",
        "        return torch.tensor(encoded_name), torch.tensor(gender, dtype=torch.long)"
      ],
      "metadata": {
        "id": "-Cequ9BEHqSv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_collate(batch):\n",
        "    # Separate names and genders\n",
        "    names = [item[0] for item in batch]\n",
        "    genders = torch.stack([item[1] for item in batch])\n",
        "\n",
        "    padded_seqs = pad_sequence(names, batch_first=True, padding_value=0)\n",
        "    input_seq = padded_seqs[:, :-1]\n",
        "    target_seq = padded_seqs[:, 1:]\n",
        "\n",
        "    return input_seq, target_seq, genders"
      ],
      "metadata": {
        "id": "a2W3RkDVIPbB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenderAwareTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n",
        "        super(GenderAwareTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gender_embed = nn.Embedding(2, embed_size)  # 2 for male/female\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, gender):\n",
        "        # Get embeddings\n",
        "        char_embeddings = self.embed(x)\n",
        "        gender_embeddings = self.gender_embed(gender).unsqueeze(1).expand(-1, x.size(1), -1)\n",
        "\n",
        "        # Combine character and gender embeddings\n",
        "        x = char_embeddings + gender_embeddings + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Q6EfjFlFIRGs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, epochs=200):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (input_seq, target_seq, genders) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq, genders)\n",
        "            loss = criterion(output.transpose(1, 2), target_seq)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        average_loss = total_loss / batch_count\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')"
      ],
      "metadata": {
        "id": "tTW3PsQjISlI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, dataset, gender, start_str='a', max_length=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_str = start_str.lower()\n",
        "\n",
        "        chars = [dataset.char_to_int[c] for c in start_str]\n",
        "        input_seq = torch.tensor(chars).unsqueeze(0)\n",
        "        gender_tensor = torch.tensor([gender])  # 0 for male, 1 for female\n",
        "\n",
        "        output_name = start_str\n",
        "        for _ in range(max_length - len(start_str)):\n",
        "            output = model(input_seq, gender_tensor)\n",
        "            probabilities = torch.softmax(output[0, -1], dim=0)\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[next_char_idx]\n",
        "\n",
        "            if next_char == ' ':\n",
        "                break\n",
        "\n",
        "            output_name += next_char\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "        return output_name.capitalize()\n"
      ],
      "metadata": {
        "id": "LxsFoJlrIU5J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset with both male and female names\n",
        "dataset = NameDataset('vyru_vardai.txt', 'moteru_vardai.txt')\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = GenderAwareTransformer(\n",
        "    vocab_size=dataset.vocab_size,\n",
        "    embed_size=128,\n",
        "    num_heads=8,\n",
        "    forward_expansion=4\n",
        ")\n",
        "train_model(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CWCP7uSIXhg",
        "outputId": "cc75a9f8-95d0-41d8-fc7b-b5c9f638a119"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.461242005287894\n",
            "Epoch 2, Average Loss: 1.3170617258124673\n",
            "Epoch 3, Average Loss: 1.2948501585971697\n",
            "Epoch 4, Average Loss: 1.2899656693925972\n",
            "Epoch 5, Average Loss: 1.276165348739021\n",
            "Epoch 6, Average Loss: 1.2772356839519245\n",
            "Epoch 7, Average Loss: 1.2679237895332307\n",
            "Epoch 8, Average Loss: 1.263676757397859\n",
            "Epoch 9, Average Loss: 1.2578558964220432\n",
            "Epoch 10, Average Loss: 1.2571673562875378\n",
            "Epoch 11, Average Loss: 1.2514584620479539\n",
            "Epoch 12, Average Loss: 1.2486138025762534\n",
            "Epoch 13, Average Loss: 1.2487417801095564\n",
            "Epoch 14, Average Loss: 1.2472308966010928\n",
            "Epoch 15, Average Loss: 1.2416137812637058\n",
            "Epoch 16, Average Loss: 1.245443407254728\n",
            "Epoch 17, Average Loss: 1.2426347400359956\n",
            "Epoch 18, Average Loss: 1.2407919378148708\n",
            "Epoch 19, Average Loss: 1.2380455395449763\n",
            "Epoch 20, Average Loss: 1.2369363765000354\n",
            "Epoch 21, Average Loss: 1.2369518025590498\n",
            "Epoch 22, Average Loss: 1.2365770851199336\n",
            "Epoch 23, Average Loss: 1.2389519238189275\n",
            "Epoch 24, Average Loss: 1.230919109031617\n",
            "Epoch 25, Average Loss: 1.2342526531031008\n",
            "Epoch 26, Average Loss: 1.2330384532453513\n",
            "Epoch 27, Average Loss: 1.232938358199455\n",
            "Epoch 28, Average Loss: 1.2278218097366362\n",
            "Epoch 29, Average Loss: 1.231211988115499\n",
            "Epoch 30, Average Loss: 1.232167043469169\n",
            "Epoch 31, Average Loss: 1.2312529857450794\n",
            "Epoch 32, Average Loss: 1.2292011957394746\n",
            "Epoch 33, Average Loss: 1.231109095420762\n",
            "Epoch 34, Average Loss: 1.2307953066505462\n",
            "Epoch 35, Average Loss: 1.2336693415528701\n",
            "Epoch 36, Average Loss: 1.228739201315778\n",
            "Epoch 37, Average Loss: 1.2325935538107227\n",
            "Epoch 38, Average Loss: 1.230522839683789\n",
            "Epoch 39, Average Loss: 1.2266924642762649\n",
            "Epoch 40, Average Loss: 1.2266415923009277\n",
            "Epoch 41, Average Loss: 1.2317704958878015\n",
            "Epoch 42, Average Loss: 1.2276043458418413\n",
            "Epoch 43, Average Loss: 1.2294594341587173\n",
            "Epoch 44, Average Loss: 1.2217639934404094\n",
            "Epoch 45, Average Loss: 1.2232821988965212\n",
            "Epoch 46, Average Loss: 1.2217181734887979\n",
            "Epoch 47, Average Loss: 1.225234120259643\n",
            "Epoch 48, Average Loss: 1.2264489658736428\n",
            "Epoch 49, Average Loss: 1.2199525854333115\n",
            "Epoch 50, Average Loss: 1.224139158904788\n",
            "Epoch 51, Average Loss: 1.2198885337637346\n",
            "Epoch 52, Average Loss: 1.223171291615181\n",
            "Epoch 53, Average Loss: 1.223913873372813\n",
            "Epoch 54, Average Loss: 1.2232740132705024\n",
            "Epoch 55, Average Loss: 1.2244001061548828\n",
            "Epoch 56, Average Loss: 1.2229069360631257\n",
            "Epoch 57, Average Loss: 1.2157532396052666\n",
            "Epoch 58, Average Loss: 1.2251934665936255\n",
            "Epoch 59, Average Loss: 1.2189849016694683\n",
            "Epoch 60, Average Loss: 1.214743020741836\n",
            "Epoch 61, Average Loss: 1.223362002919314\n",
            "Epoch 62, Average Loss: 1.2204587501970676\n",
            "Epoch 63, Average Loss: 1.217107669402488\n",
            "Epoch 64, Average Loss: 1.2174159297830032\n",
            "Epoch 65, Average Loss: 1.2240183928267288\n",
            "Epoch 66, Average Loss: 1.2213911112589326\n",
            "Epoch 67, Average Loss: 1.2207556644918418\n",
            "Epoch 68, Average Loss: 1.216539663288433\n",
            "Epoch 69, Average Loss: 1.2182052187297656\n",
            "Epoch 70, Average Loss: 1.2180617319736555\n",
            "Epoch 71, Average Loss: 1.224296313500687\n",
            "Epoch 72, Average Loss: 1.218024592625765\n",
            "Epoch 73, Average Loss: 1.2233871838792039\n",
            "Epoch 74, Average Loss: 1.2142018890192385\n",
            "Epoch 75, Average Loss: 1.2181808109811172\n",
            "Epoch 76, Average Loss: 1.2206075158514995\n",
            "Epoch 77, Average Loss: 1.2193470210897122\n",
            "Epoch 78, Average Loss: 1.2161091771050405\n",
            "Epoch 79, Average Loss: 1.2183005435193481\n",
            "Epoch 80, Average Loss: 1.2184323437600268\n",
            "Epoch 81, Average Loss: 1.2189409023688245\n",
            "Epoch 82, Average Loss: 1.2184475832305879\n",
            "Epoch 83, Average Loss: 1.213374231879419\n",
            "Epoch 84, Average Loss: 1.2165314423236921\n",
            "Epoch 85, Average Loss: 1.21184625220393\n",
            "Epoch 86, Average Loss: 1.218233358718661\n",
            "Epoch 87, Average Loss: 1.2166738338150054\n",
            "Epoch 88, Average Loss: 1.2145264513878955\n",
            "Epoch 89, Average Loss: 1.2132941818991196\n",
            "Epoch 90, Average Loss: 1.2119949596672661\n",
            "Epoch 91, Average Loss: 1.2125019263844246\n",
            "Epoch 92, Average Loss: 1.2123771929458196\n",
            "Epoch 93, Average Loss: 1.2197959769384663\n",
            "Epoch 94, Average Loss: 1.2120438776468572\n",
            "Epoch 95, Average Loss: 1.2156986867486252\n",
            "Epoch 96, Average Loss: 1.2101324532814177\n",
            "Epoch 97, Average Loss: 1.2153313287162026\n",
            "Epoch 98, Average Loss: 1.209774898681716\n",
            "Epoch 99, Average Loss: 1.2140752397507075\n",
            "Epoch 100, Average Loss: 1.2130271290601948\n",
            "Epoch 101, Average Loss: 1.2064335812693057\n",
            "Epoch 102, Average Loss: 1.2150332861738242\n",
            "Epoch 103, Average Loss: 1.213485239287139\n",
            "Epoch 104, Average Loss: 1.217751782166628\n",
            "Epoch 105, Average Loss: 1.217163932653284\n",
            "Epoch 106, Average Loss: 1.2131869802361892\n",
            "Epoch 107, Average Loss: 1.2155173635294314\n",
            "Epoch 108, Average Loss: 1.2152214900778215\n",
            "Epoch 109, Average Loss: 1.2133940192079355\n",
            "Epoch 110, Average Loss: 1.2117704178975977\n",
            "Epoch 111, Average Loss: 1.2164460392337544\n",
            "Epoch 112, Average Loss: 1.2128368064820059\n",
            "Epoch 113, Average Loss: 1.2123195752795977\n",
            "Epoch 114, Average Loss: 1.2137892677378748\n",
            "Epoch 115, Average Loss: 1.2076592895353264\n",
            "Epoch 116, Average Loss: 1.216362965907975\n",
            "Epoch 117, Average Loss: 1.2163516584592375\n",
            "Epoch 118, Average Loss: 1.2118652750380896\n",
            "Epoch 119, Average Loss: 1.2080821262989119\n",
            "Epoch 120, Average Loss: 1.2123596687090727\n",
            "Epoch 121, Average Loss: 1.2108150982102859\n",
            "Epoch 122, Average Loss: 1.2103770714503503\n",
            "Epoch 123, Average Loss: 1.2116343091599084\n",
            "Epoch 124, Average Loss: 1.2126505495060103\n",
            "Epoch 125, Average Loss: 1.2120670558435644\n",
            "Epoch 126, Average Loss: 1.207968571676096\n",
            "Epoch 127, Average Loss: 1.2049227392249429\n",
            "Epoch 128, Average Loss: 1.2117754190335632\n",
            "Epoch 129, Average Loss: 1.2116619697201394\n",
            "Epoch 130, Average Loss: 1.2088800143347427\n",
            "Epoch 131, Average Loss: 1.2144461929091352\n",
            "Epoch 132, Average Loss: 1.2111076712608337\n",
            "Epoch 133, Average Loss: 1.2062582046146921\n",
            "Epoch 134, Average Loss: 1.2105205249880613\n",
            "Epoch 135, Average Loss: 1.2101594803361553\n",
            "Epoch 136, Average Loss: 1.214139605463729\n",
            "Epoch 137, Average Loss: 1.215000792454354\n",
            "Epoch 138, Average Loss: 1.2063020901246504\n",
            "Epoch 139, Average Loss: 1.204679687268178\n",
            "Epoch 140, Average Loss: 1.2087220858208276\n",
            "Epoch 141, Average Loss: 1.2090019840496802\n",
            "Epoch 142, Average Loss: 1.2092947839748247\n",
            "Epoch 143, Average Loss: 1.206477415420321\n",
            "Epoch 144, Average Loss: 1.2094005197875584\n",
            "Epoch 145, Average Loss: 1.2091006311503323\n",
            "Epoch 146, Average Loss: 1.2094768608511672\n",
            "Epoch 147, Average Loss: 1.205335155070535\n",
            "Epoch 148, Average Loss: 1.2078395151338088\n",
            "Epoch 149, Average Loss: 1.209605277998174\n",
            "Epoch 150, Average Loss: 1.2115693158311807\n",
            "Epoch 151, Average Loss: 1.2118806980344146\n",
            "Epoch 152, Average Loss: 1.2049000300908748\n",
            "Epoch 153, Average Loss: 1.213229327098183\n",
            "Epoch 154, Average Loss: 1.2104811347991582\n",
            "Epoch 155, Average Loss: 1.2063244279665437\n",
            "Epoch 156, Average Loss: 1.2045705862667249\n",
            "Epoch 157, Average Loss: 1.2087815967001934\n",
            "Epoch 158, Average Loss: 1.210450787553674\n",
            "Epoch 159, Average Loss: 1.2118540554649746\n",
            "Epoch 160, Average Loss: 1.2054047626940159\n",
            "Epoch 161, Average Loss: 1.212995350596462\n",
            "Epoch 162, Average Loss: 1.2113974431286687\n",
            "Epoch 163, Average Loss: 1.21124330080545\n",
            "Epoch 164, Average Loss: 1.2103527134115046\n",
            "Epoch 165, Average Loss: 1.21131768905127\n",
            "Epoch 166, Average Loss: 1.2106277945013386\n",
            "Epoch 167, Average Loss: 1.2083333445631939\n",
            "Epoch 168, Average Loss: 1.2064845839979148\n",
            "Epoch 169, Average Loss: 1.2088252473254448\n",
            "Epoch 170, Average Loss: 1.2100481909254324\n",
            "Epoch 171, Average Loss: 1.208628256330377\n",
            "Epoch 172, Average Loss: 1.2087322407560386\n",
            "Epoch 173, Average Loss: 1.205503788154587\n",
            "Epoch 174, Average Loss: 1.2075730925020964\n",
            "Epoch 175, Average Loss: 1.2032899849499639\n",
            "Epoch 176, Average Loss: 1.2081072516592124\n",
            "Epoch 177, Average Loss: 1.2060748616697288\n",
            "Epoch 178, Average Loss: 1.2031567238065093\n",
            "Epoch 179, Average Loss: 1.2090560836283115\n",
            "Epoch 180, Average Loss: 1.2038636499714004\n",
            "Epoch 181, Average Loss: 1.205438223045334\n",
            "Epoch 182, Average Loss: 1.2060206730375176\n",
            "Epoch 183, Average Loss: 1.2120973477250503\n",
            "Epoch 184, Average Loss: 1.2023122817160112\n",
            "Epoch 185, Average Loss: 1.2046691782389705\n",
            "Epoch 186, Average Loss: 1.2030364955838018\n",
            "Epoch 187, Average Loss: 1.2100731758732097\n",
            "Epoch 188, Average Loss: 1.206090205978499\n",
            "Epoch 189, Average Loss: 1.2090183668457002\n",
            "Epoch 190, Average Loss: 1.2086336273449683\n",
            "Epoch 191, Average Loss: 1.2096071490657188\n",
            "Epoch 192, Average Loss: 1.2037102104175703\n",
            "Epoch 193, Average Loss: 1.2102165384726091\n",
            "Epoch 194, Average Loss: 1.2068975907069421\n",
            "Epoch 195, Average Loss: 1.202624374463153\n",
            "Epoch 196, Average Loss: 1.2038613736865078\n",
            "Epoch 197, Average Loss: 1.2092919575838232\n",
            "Epoch 198, Average Loss: 1.2072420560795327\n",
            "Epoch 199, Average Loss: 1.2088257479573428\n",
            "Epoch 200, Average Loss: 1.2059660843709712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate examples\n",
        "print(\"Vyru vardai:\")\n",
        "for _ in range(5):\n",
        "    name = sample(model, dataset, gender=0, start_str='f')\n",
        "    print(name)\n",
        "\n",
        "print(\"\\nMoteru vardai:\")\n",
        "for _ in range(5):\n",
        "    name = sample(model, dataset, gender=1, start_str='L')\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQZ8ZxGMIZc3",
        "outputId": "c15f0ca5-e86b-4785-b53e-3947ea47de9c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vyru vardai:\n",
            "Fruelis\n",
            "Fugovius\n",
            "Fìlvintas\n",
            "Flažydo\n",
            "Fèdodas\n",
            "\n",
            "Moteru vardai:\n",
            "Lice\n",
            "Lafrygė\n",
            "Ludùzana\n",
            "Labetndà\n",
            "Lĩskonda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), 'name_model.pt')\n",
        "\n",
        "# Save mappings\n",
        "mappings = {\n",
        "    'char_to_int': dataset.char_to_int,\n",
        "    'int_to_char': {str(k): v for k, v in dataset.int_to_char.items()},\n",
        "    'vocab_size': dataset.vocab_size\n",
        "}\n",
        "import json\n",
        "with open('name_mappings.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(mappings, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "MgpE3-imQKn7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "padaryt kad lowercase butu, o output pirma raide auto padidina. istrainint ant daugiau epochu"
      ],
      "metadata": {
        "id": "TgFYgHd4mG9-"
      }
    }
  ]
}